### LLM Settings

The LLM Settings tab provides advanced configuration options for the language models used throughout STRIKE:

1. **Model Configuration**:
   - Add, edit, and remove language models
   - Configure model parameters including:
     - Provider (OpenAI, Azure OpenAI, Anthropic, Google AI/Gemini)
     - Model name (e.g., gpt-4, claude-3-opus)
     - API key and custom endpoints
     - Temperature and max tokens
   - Set default models for different features
   - Manage API credentials

2. **Usage Monitoring**:
   - Track token usage by model
   - Monitor costs and request volume
   - View usage trends over time (daily, weekly, monthly)
   - Export usage reports

3. **Performance Settings**:
   - Configure caching behavior
   - Set timeout parameters
   - Adjust rate limiting
   - Configure fallback behavior

4. **Content Policies**:
   - Set content filtering rules
   - Configure moderation settings
   - Define acceptable use parameters
   - Set up content warnings

5. **Mock Data Integration**:
   - When mock data is enabled, the system displays simulated usage data
   - This allows testing and demonstration without incurring actual API costs
   - You can enable mock data in the **Development Mode** section
   - A note appears on the Usage tab when viewing mock data

For detailed information on managing language models, see the [Language Model Management](#language-model-management) section. 